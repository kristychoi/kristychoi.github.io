<!DOCTYPE html>
<html lang="en">
	<head>
		<title>Kristy Choi</title>
		<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
		<link rel="stylesheet" href="css/main.css">
		<script>
			(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
			(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
			})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

			ga('create', 'UA-103320786-1', 'auto');
			ga('send', 'pageview');
		</script>
	</head>
	<body>
		<div class="container">
			<div class="page-header">
				<h2>Kristy Choi 
				<p><small><a href="mailto:kechoi@cs.stanford.edu">kechoi [at] cs.stanford.edu</a></small></h2></p>
				<p class="profpic"><img src="images/cropped_pic.jpg"
				width="225px" /></p>
				<p><a href="https://github.com/kristychoi">Github</a>
				|Â <a href="https://drive.google.com/file/d/1VIPbG-7mfhFt2f9BXE-UNM9_nqQysSmU/view?usp=sharing">CV</a> | 
				  <a href="https://scholar.google.com/citations?user=WetKfYoAAAAJ&hl=en">Google Scholar</a>
				| <a href="/blog/">Blog</a>
				</p>
			</div>
			<!-- <p><span style="color:darkred"><strong>I am currently on the job market for the 2022-2023 cycle!</strong></span></p> -->

			<p> I am currently a Research Scientist on the Llama Post-training team at Meta Gen AI.</p>

			<p>I received my Ph.D. in Computer Science at Stanford University advised by <a href="https://cs.stanford.edu/~ermon/">Stefano Ermon</a>, where I was affiliated with the <a href="https://ai.stanford.edu/">SAIL</a> and <a href="http://statsml.stanford.edu/">StatML</a> groups. My research is centered around machine learning with limited labeled supervision, and is currently focused on developing techniques for better adaptation and controllability in deep generative models. I ground my methodological work in societal applications motivated by problems in personalization and fairness.</p>

			<p>My research was supported by the <a href="https://www.nsfgrfp.org/">NSF GRFP</a>, <a href="https://vpge.stanford.edu/fellowships-funding/sgf">Stanford Graduate Fellowship</a>, the <a href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship">Qualcomm Innovation Fellowship</a>, and the <a href="https://www.twosigma.com/academic-outreach/">Two Sigma Diversity PhD Fellowship</a>. I completed my undergraduate studies in CS-Stats at Columbia, where I worked on problems in computational biology as part of the <a href="https://www.mskcc.org/research-areas/labs/dana-pe-er">Pe'er lab</a>.</p>

			<p>I previously interned at <a href="https://ai.google/research/teams/brain/">Google
			Brain</a> in 2019 as part of the <a href="https://magenta.tensorflow.org/">Magenta</a> project. In my free time I'm an avid tennis player, runner, and food enthusiast!

			<!-- <h3>News</h3>
			<dl>
				<i>Nov 2022:</i> Hi
			</dl> -->

			<h3>Preprints</h3>
			<dl>
				<dt>EMMA: End-to-End Multimodal Model for Autonomous Driving.</dt>
				<dd>Jyh-Jing Hwang, Runsheng Xu, Hubert Lin, Wei-Chih Hung, Jingwei Ji, <b>Kristy Choi</b>, Di Huang, Tong He, Paul Covington, Benjamin Sapp, James Guo, Dragomir Anguelov, Mingxing Tan</dd>
				<dd><i>arXiv, 2024.</i></dd>
				<dd><a href="https://arxiv.org/abs/2410.23262v1">[arXiv]</a><a href="https://waymo.com/blog/2024/10/introducing-emma/">[blog]</a></dd>
			</dl>
			<dl>
				<dt>LMPriors: Pre-Trained Language Models as Task-Specific Priors.</dt>
				<dd><b>Kristy Choi*</b>, Chris Cundy*, Sanjari Srivastava, Stefano Ermon</dd>
				<dd><i>Foundation Models for Decision Making Workshop, NeurIPS 2022.</i></dd>
				<dd><a href="https://arxiv.org/abs/2210.12530">[arXiv]</a>[code soon]</dd>
			</dl>
<!-- 			<dl>
				<dt>Tensor Decomposition for Single-cell RNA-seq Data.</dt>
				<dd><b>Kristy Choi*</b>, Ambrose J. Carr*, Sandhya Prabhakaran, Dana Pe'er</dd>
				<dd><i>Practical Bayesian Nonparametrics Workshop, NeurIPS 2016. </i></dd>
				<dd><a href="https://drive.google.com/file/d/0B3WHb3BabixAb2RfMmc4eUJjUE0/view">[pdf]</a></dd>
			</dl> -->

			<h3>Publications</h3>
			<dl>
				<dt>Neural Network Compression for Noisy Storage Devices.</dt>
				<dd>Berivan Isik, <b>Kristy Choi</b>, Xin Zheng, Tsachy Weissman, Stefano Ermon, H.-S. Philip Wong, Armin Alaghi</dd>
				<dd><i>ACM Transactions on Embedded Computing Systems, 2023.</i></dd>
				<dd><a href="https://arxiv.org/abs/2102.07725">[arXiv]</a>[code soon]</dd>
			</dl>
			<dl>
				<dt>Concrete Score Matching: Generalized Score Matching for Discrete Data.</dt>
				<dd>Chenlin Meng*, <b>Kristy Choi*</b>, Jiaming Song, Stefano Ermon</dd>
				<dd><i>Neural Information Processing Systems (NeurIPS), 2022.</i></dd>
				<dd><a href="https://arxiv.org/abs/2211.00802">[pdf]</a>[code soon]</dd>
			</dl>
			<dl>
				<dt>ButterflyFlow: Building Invertible Layers with Butterfly Matrices.</dt>
				<dd>Chenlin Meng*, Linqi Zhou*, <b>Kristy Choi*</b>, Tri Dao, Stefano
					Ermon</dd>
				<dd><i>International Conference of Machine Learning (ICML), 2022.</i></dd>
				<dd><a href="https://proceedings.mlr.press/v162/meng22a/meng22a.pdf">[pdf]</a>[code soon]</dd>
			</dl>
			<dl>
				<dt>Density Ratio Estimation via Infinitesimal Classification.</dt>
				<dd><b>Kristy Choi*</b>, Chenlin Meng*, Yang Song, Stefano
					Ermon</dd>
				<dd><i>International Conference on Artificial Intelligence and Statistics (AISTATS), 2022.</i></dd>
				<dd><span style="color:darkred"><strong>Oral presentation [Top 2.6%]</strong></span></dd>
				<dd><a href="https://arxiv.org/abs/2111.11010">[arXiv]</a><a href="https://github.com/ermongroup/dre-infinity">[code]</a></dd>
			</dl>
			<dl>
				<dt>Featurized Density Ratio Estimation.</dt>
				<dd><b>Kristy Choi*</b>, Madeline Liao*, Stefano Ermon</dd>
				<dd><i>Uncertainty in Artificial Intelligence (UAI), 2021.</i></dd>
				<dd><a href="https://arxiv.org/abs/2107.02212">[arXiv]</a><a href="https://github.com/ermongroup/f-dre">[code]</a></dd>
			</dl>
			<dl>
				<dt>Robust Representation Learning via Perceptual Similarity Metrics.</dt>
				<dd>Saeid Taghanaki*, <b>Kristy Choi*</b>, Amir Khasahmadi, Anirudh Goyal</dd>
				<dd><i>International Conference of Machine Learning (ICML), 2021.</i></dd>
				<dd><a href="https://arxiv.org/pdf/2106.06620.pdf">[arXiv]</a>[code soon]</dd>
			</dl>
			<dl>
				<dt>Encoding Musical Style with Transformer Autoencoders.</dt>
				<dd><b>Kristy Choi</b>, Curtis Hawthorne, Ian Simon, Monica Dinculescu, Jesse Engel</dd>
				<dd><i>International Conference of Machine Learning (ICML), 2020.</i></dd>
				<dd><a href="http://arxiv.org/abs/1912.05537">[arXiv]</a>
				<a href="https://github.com/magenta/magenta/tree/master/magenta/models/score2perf">[code]</a></dd>
			</dl>
			<dl>
				<dt>Fair Generative Modeling via Weak Supervision.</dt>
				<dd><b>Kristy Choi*</b>, Aditya Grover*, Trisha Singh, Rui Shu, Stefano Ermon</dd>
				<dd><i>International Conference of Machine Learning (ICML), 2020.</i></dd>
				<dd><a href="https://arxiv.org/abs/1910.12008">[arXiv]</a>
				<a href="https://github.com/ermongroup/fairgen">[code]</a>
				<a href="https://slate.com/technology/2020/09/synthetic-data-artificial-intelligence-bias.html">[press]</a>
			</dd>
			</dl>
			<dl>
				<dt>Meta-Amortized Variational Inference and Learning.</dt>
				<dd>Mike Wu*, <b>Kristy Choi*</b>, Noah Goodman, Stefano Ermon</dd>
				<dd>AAAI Conference on Artificial Intelligence (AAAI), 2020.</dd>
<!-- 				<dd><i>Bayesian Deep Learning Workshop, NeurIPS 2019. </i><strong>Spotlight.</strong></dd> -->
				<dd><a href="https://arxiv.org/abs/1902.01950">[arXiv]</a></dd>
			</dl>
			<dl>
				<dt>Neural Joint-Source Channel Coding.</dt>
				<dd><b>Kristy Choi</b>, Kedar Tatwawadi, Aditya Grover, Tsachy Weissman, Stefano Ermon.</dd>
				<dd><i>International Conference of Machine Learning (ICML), 2019.</i></dd>
				<dd><span style="color:darkred"><strong>Oral Presentation</strong></span></dd>
				<dd><a href="http://proceedings.mlr.press/v97/choi19a/choi19a.pdf">[pdf]</a>
				<a href="https://github.com/ermongroup/necst">[code]</a></dd>
			</dl>
			<dl>
				<dt>Single-cell map of diverse immune phenotypes in the breast tumor microenvironment.</dt>
				<dd>Elham Azizi, Ambrose Carr, George Plitas, Andrew Cornish, Catherine Konopacki, Sandhya Prabhakaran, Juozas Nainys, Kenmin Wu, Vaidotas Kiseliovas,</br> Manu Setty, <b>Kristy Choi</b>, Rachel Fromme, Phuong Dao, Peter McKenney, Ruby Wasti, Krishna Kadaveru, Linas Mazutis, Alexander Rudensky, Dana Pe'er.</dd>
				<dd><i>Cell, 174(5), 1293-1308, 2018.</i></dd>
				<dd><a href="https://pubmed.ncbi.nlm.nih.gov/29961579/">[pdf]</a></dd>
			</dl>
			<dl>
				<dt>Wishbone identifies bifurcating developmental trajectories from single-cell data.</dt>
				<dd>Manu Setty, Michelle Tadmor, Shlomit Reich-Zeliger, Omer Angel, Tomer Salame, Pooja Kathail, <b>Kristy Choi</b>, Sean Bendall, Nir Friedman, Dana Pe'er.</dd>
				<dd><i>Nature Biotechnology, 34(6), 637-645, 2016.</i></dd>
				<dd><a href="http://www.nature.com/nbt/journal/v34/n6/full/nbt.3569.html">[pdf]</a></dd>
			</dl>

			<h3>Workshop Papers</h3>
			<dl>
				<dt>Tensor Decomposition for Single-cell RNA-seq Data.</dt>
				<dd><b>Kristy Choi*</b>, Ambrose J. Carr*, Sandhya Prabhakaran, Dana Pe'er</dd>
				<dd><i>Practical Bayesian Nonparametrics Workshop, NeurIPS 2016. </i></dd>
				<dd><a href="https://drive.google.com/file/d/0B3WHb3BabixAb2RfMmc4eUJjUE0/view">[pdf]</a></dd>
			</dl>

			<h3>Teaching</h3>
			<p><strong>Fall 2019:</strong> Head Teaching Assistant
			for <a href="https://deepgenerativemodels.github.io/">CS236: Deep Generative Models</a> at Stanford<p>
			<p><strong>Fall 2018:</strong> Teaching Assistant
			for CS236: Deep Generative Models at Stanford<p>
			<p><strong>Spring 2017:</strong> Head Teaching Assistant
			for COMS4117: Machine Learning at Columbia<p>

			<h3>Service</h3>
			<p><strong>Reviewer</strong>: TMLR, ICLR (2019-2023), ICML (2020-2022), NeurIPS (2019-2022), AISTATS 2022, UAI 2020, AAAI 2020.<p>
			<p><strong>Workshop Co-Organizer</strong>: <a
				href="https://wimlworkshop.org/neurips2020/">Women in Machine
															  Learning @
			NeurIPS 2020</a>; Information Theory & Machine Learning (NeurIPS
			2019)
			<p><strong>Leadership</strong>: Women in Machine Learning, Board of Directors (2022 - 2023)
			<br><br>
		</div>
	</body>
</html>
